<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tleukhan's Webpage</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>

<!-- Navigation Bar -->
<nav class="navbar">
    <div class="container">
        <div class="logo">Tleukhan Mussin</div>
        <div class="menu">
            <a href="index.html">Home</a>
            <a href="cv.html">CV</a>
            <a href="projects.html" class="active">Projects</a> <!-- Active state for Projects page -->
            <a href="publications.html">Publications</a>
            <a href="contact.html">Contact</a>
        </div>
    </div>
</nav>

<!-- Project Details Section -->
<div class="project-details container">
    <h1>NUSense: Robust Soft Optical Tactile Sensor</h1>

    <!-- Links to GitHub and Publication -->
    <div class="project-links">
        <a href="https://github.com/yourusername/project-repo" target="_blank" class="link-button">
            View on GitHub
        </a>
        <a href="https://yourpublicationlink.com" target="_blank" class="link-button">
            View Publication
        </a>
    </div>
     
    <!-- New Image Container -->
     <img src="project2/fig1_hope.png" alt="Event-based Agile Object Catching" class="project2-details-image">

    <p class="project-date">Project Date: May 29, 2023</p>

    <p>
        In this project, we explore the use of event-based cameras to improve the performance of high-speed catching tasks with quadrupedal robots. 
        Event-based cameras are specialized sensors that capture changes in the scene asynchronously, enabling faster response times and lower 
        latency compared to traditional frame-based cameras.
    </p>

    <h2>Event Preprocessing Summary</h2>
    <p>The event preprocessing pipeline for DVS (Dynamic Vision Sensor) data converts raw, continuous event streams into a structured, graph-based format suitable for 
        Graph Neural Networks (GNNs). Hereâ€™s a summary of each stage:</p>

    <h3>1. Denoising</h3>
    <p>Isolated, noisy events are removed by applying a space-time filter. This step eliminates spurious events that don't have nearby events in space and time, 
        improving data quality and reducing the likelihood of overfitting.</p>

    <h3>2. Time Window Selection</h3>
    <p>The continuous event stream is divided into small, fixed-duration time windows. Each time window captures the events occurring within that timeframe, 
        effectively segmenting the stream into discrete chunks. Each time window serves as a snapshot, which becomes the basis for creating a separate graph.</p>

    <h3>3. Sub-sampling and Time Normalization</h3>
    <p>Within each time window, events are sub-sampled to reduce data density and computational load. Time normalization is then applied to standardize the 
        temporal scale across windows, ensuring consistency in the time representation for each graph.</p>

    <h3>4. Edge Creation</h3>
    <p>Events within each time window are connected to form a graph. A radius-neighborhood algorithm creates edges between nodes (events) that are spatially and temporally close within the window. 
        This captures local spatio-temporal relationships among events in that timeframe.</p>

    <p>The result is a sequence of graphs, each representing the spatio-temporal structure of events within a single time window. 
        This graph sequence enables the GNN to learn dynamic patterns across time, making it effective for tasks like gesture recognition in event-based vision systems.</p>

    <h4>Technologies Used</h4>
    <ul>
        <li>Event-based Vision Sensors</li>
        <li>ROS (Robot Operating System)</li>
        <li>Python and C++ for Algorithm Development</li>
        <li>Real-time Control Systems</li>
        <li>Machine Learning for Object Detection</li>
    </ul>

    <h5>Project Gallery</h5>
    <div class="project-gallery">
        <img src="project2/euclidean_distances.png" alt="Robot Catching Object" class="gallery-image">
        <img src="project2/assemble sensor1.png" alt="Event-based Vision in Action" class="gallery-image">
        <img src="project2/control_points_image2.png" alt="High-Speed Object Detection" class="gallery-image">
    </div>

</div>

</body>
</html>

